{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffb08bcf",
   "metadata": {},
   "source": [
    "Text pre-processing level-1 tokenization, stemming, lemmatization,stop word removal and PoS tagging using NLTK\n",
    "\n",
    "\n",
    "References:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9370f21e",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf2d82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f25df508",
   "metadata": {},
   "outputs": [],
   "source": [
    "para= \"\"\"India is the largest democratic country.\n",
    "It is a big country divided into 29 states and 7 union territories.These states and union territories have been created so that the government can run the country more easily. India also has many different kinds of physical features in different parts of the country \n",
    "that are spread over its states and union territories.India is a very diverse country as well, which means that the people around the country are different in many ways. Even though India is such a diverse place, it is united as one country.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e49c3e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['India is the largest democratic country.', 'It is a big country divided into 29 states and 7 union territories.These states and union territories have been created so that the government can run the country more easily.', 'India also has many different kinds of physical features in different parts of the country \\nthat are spread over its states and union territories.India is a very diverse country as well, which means that the people around the country are different in many ways.', 'Even though India is such a diverse place, it is united as one country.']\n"
     ]
    }
   ],
   "source": [
    "#Sentence tokenize\n",
    "sentences= sent_tokenize(para)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b417bc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['India', 'is', 'the', 'largest', 'democratic', 'country', '.']\n"
     ]
    }
   ],
   "source": [
    "#word Tokenize\n",
    "words= word_tokenize(sentences[0])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d8877f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['India', 'is', 'the', 'largest', 'democratic', 'country', '.', 'It', 'is', 'a', 'big', 'country', 'divided', 'into', '29', 'states', 'and', '7', 'union', 'territories.These', 'states', 'and', 'union', 'territories', 'have', 'been', 'created', 'so', 'that', 'the', 'government', 'can', 'run', 'the', 'country', 'more', 'easily', '.', 'India', 'also', 'has', 'many', 'different', 'kinds', 'of', 'physical', 'features', 'in', 'different', 'parts', 'of', 'the', 'country', 'that', 'are', 'spread', 'over', 'its', 'states', 'and', 'union', 'territories.India', 'is', 'a', 'very', 'diverse', 'country', 'as', 'well', ',', 'which', 'means', 'that', 'the', 'people', 'around', 'the', 'country', 'are', 'different', 'in', 'many', 'ways', '.', 'Even', 'though', 'India', 'is', 'such', 'a', 'diverse', 'place', ',', 'it', 'is', 'united', 'as', 'one', 'country', '.']\n"
     ]
    }
   ],
   "source": [
    "#Using extend\n",
    "words_token=[]\n",
    "for sent in sentences:\n",
    "    words_token.extend(word_tokenize(sent))\n",
    "print(words_token)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "faeb6bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['India', 'is', 'the', 'largest', 'democratic', 'country', '.'], ['It', 'is', 'a', 'big', 'country', 'divided', 'into', '29', 'states', 'and', '7', 'union', 'territories.These', 'states', 'and', 'union', 'territories', 'have', 'been', 'created', 'so', 'that', 'the', 'government', 'can', 'run', 'the', 'country', 'more', 'easily', '.'], ['India', 'also', 'has', 'many', 'different', 'kinds', 'of', 'physical', 'features', 'in', 'different', 'parts', 'of', 'the', 'country', 'that', 'are', 'spread', 'over', 'its', 'states', 'and', 'union', 'territories.India', 'is', 'a', 'very', 'diverse', 'country', 'as', 'well', ',', 'which', 'means', 'that', 'the', 'people', 'around', 'the', 'country', 'are', 'different', 'in', 'many', 'ways', '.'], ['Even', 'though', 'India', 'is', 'such', 'a', 'diverse', 'place', ',', 'it', 'is', 'united', 'as', 'one', 'country', '.']]\n"
     ]
    }
   ],
   "source": [
    "#Useing append\n",
    "words_token=[]\n",
    "for sent in sentences:\n",
    "    words_token.append(word_tokenize(sent))\n",
    "print(words_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ab3d445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['India', 'is', 'the', 'largest', 'democratic', 'country', '.', 'It', 'is', 'a', 'big', 'country', 'divided', 'into', '29', 'states', 'and', '7', 'union', 'territories.These', 'states', 'and', 'union', 'territories', 'have', 'been', 'created', 'so', 'that', 'the', 'government', 'can', 'run', 'the', 'country', 'more', 'easily', '.', 'India', 'also', 'has', 'many', 'different', 'kinds', 'of', 'physical', 'features', 'in', 'different', 'parts', 'of', 'the', 'country', 'that', 'are', 'spread', 'over', 'its', 'states', 'and', 'union', 'territories.India', 'is', 'a', 'very', 'diverse', 'country', 'as', 'well', ',', 'which', 'means', 'that', 'the', 'people', 'around', 'the', 'country', 'are', 'different', 'in', 'many', 'ways', '.', 'Even', 'though', 'India', 'is', 'such', 'a', 'diverse', 'place', ',', 'it', 'is', 'united', 'as', 'one', 'country', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(para))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc524d2",
   "metadata": {},
   "source": [
    "### Change case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a22b378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "india is the largest democratic country.\n",
      "it is a big country divided into 29 states and 7 union territories.these states and union territories have been created so that the government can run the country more easily. india also has many different kinds of physical features in different parts of the country \n",
      "that are spread over its states and union territories.india is a very diverse country as well, which means that the people around the country are different in many ways. even though india is such a diverse place, it is united as one country.\n"
     ]
    }
   ],
   "source": [
    "#Convert all words to lower case\n",
    "\n",
    "para_lower= para.lower()\n",
    "print(para_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "597d3444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDIA IS THE LARGEST DEMOCRATIC COUNTRY.\n",
      "IT IS A BIG COUNTRY DIVIDED INTO 29 STATES AND 7 UNION TERRITORIES.THESE STATES AND UNION TERRITORIES HAVE BEEN CREATED SO THAT THE GOVERNMENT CAN RUN THE COUNTRY MORE EASILY. INDIA ALSO HAS MANY DIFFERENT KINDS OF PHYSICAL FEATURES IN DIFFERENT PARTS OF THE COUNTRY \n",
      "THAT ARE SPREAD OVER ITS STATES AND UNION TERRITORIES.INDIA IS A VERY DIVERSE COUNTRY AS WELL, WHICH MEANS THAT THE PEOPLE AROUND THE COUNTRY ARE DIFFERENT IN MANY WAYS. EVEN THOUGH INDIA IS SUCH A DIVERSE PLACE, IT IS UNITED AS ONE COUNTRY.\n"
     ]
    }
   ],
   "source": [
    "#Convert all words to upper case\n",
    "\n",
    "para_upper= para.upper()\n",
    "print(para_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a56900",
   "metadata": {},
   "source": [
    "### Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f03c7eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ami.munshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14260828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5bdca6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'had', 'how', \"haven't\", 'when', 'below', 'but', 'you', 'are', 'ours', 'then', 'll', \"isn't\", 'ma', 'herself', 'above', 'if', 'other', 'that', 'can', 'again', 'aren', 'they', 'he', 'does', 'each', 'yourselves', \"hadn't\", 'some', 'weren', 'than', 's', 'your', 'shan', 'an', 'against', 'hadn', 'haven', 'has', \"didn't\", 'or', \"she's\", 'more', 'having', 'while', 'should', 'out', 'up', 'such', 'too', 'am', 'both', 'have', 'just', 'and', 'through', 'because', 'few', 'm', 'his', 'isn', 'we', 'the', 'didn', 'which', 'what', 't', 'was', 'any', 'who', 'our', 'itself', \"wouldn't\", 'into', 'after', 'mightn', 'shouldn', 'under', \"don't\", 'those', 'over', 'whom', 'down', 'himself', \"it's\", 'y', 'couldn', 'during', 'o', \"weren't\", 'him', \"couldn't\", 'their', 'be', 'now', 'd', 'between', 'in', \"won't\", 'of', 'me', 'as', 'for', 'about', 'why', 'own', 'wasn', 'wouldn', 'only', 'there', 'did', 'to', \"hasn't\", 'where', 'here', 'on', \"you'll\", 'been', 'being', 'will', 'she', 'all', \"mightn't\", 'before', 'from', 'these', 'nor', \"shouldn't\", 'until', 'them', 've', 'don', 'i', 'its', 'yours', 'doing', 'my', 'needn', 'so', 'hers', 'with', 'it', 'at', 'ain', 'once', \"you're\", \"wasn't\", \"shan't\", 'most', 'won', 'by', \"that'll\", 'same', 'hasn', \"mustn't\", 'very', 'were', 'doesn', \"needn't\", 'theirs', 'mustn', 'is', \"you've\", 'this', 'yourself', \"doesn't\", 'off', 'ourselves', 'a', 'further', 'themselves', \"you'd\", \"should've\", \"aren't\", 'no', 're', 'do', 'her', 'myself', 'not'}\n"
     ]
    }
   ],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20f16350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['india', 'largest', 'democratic', 'country', '.', 'big', 'country', 'divided', '29', 'states', '7', 'union', 'territories.these', 'states', 'union', 'territories', 'created', 'government', 'run', 'country', 'easily', '.', 'india', 'also', 'many', 'different', 'kinds', 'physical', 'features', 'different', 'parts', 'country', 'spread', 'states', 'union', 'territories.india', 'diverse', 'country', 'well', ',', 'means', 'people', 'around', 'country', 'different', 'many', 'ways', '.', 'even', 'though', 'india', 'diverse', 'place', ',', 'united', 'one', 'country', '.']\n"
     ]
    }
   ],
   "source": [
    "word_filtered=[]\n",
    "para_lower_tokenize=word_tokenize(para_lower)\n",
    "\n",
    "for word in para_lower_tokenize:\n",
    "    if word not in stopWords:\n",
    "        word_filtered.append(word)\n",
    "print(word_filtered)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4af63085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length before and after stop word removal is: 100 and 58 respectively\n"
     ]
    }
   ],
   "source": [
    "print(\"length before and after stop word removal is: {} and {} respectively\".format(len(para_lower_tokenize), len(word_filtered)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eba588",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Text processing task in which you reduce words to their root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "809c9d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44e567f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait  :  wait\n",
      "waiting  :  wait\n",
      "waited  :  wait\n",
      "waits  :  wait\n",
      "listening  :  listen\n",
      "history  :  histori\n",
      "abitlity  :  abitl\n",
      "arrival  :  arriv\n",
      "finally  :  final\n",
      "congratulations  :  congratul\n",
      "exaggeration  :  exagger\n",
      "understandable  :  understand\n",
      "probability  :  probabl\n",
      "player  :  player\n",
      "toys  :  toy\n",
      "consumer  :  consum\n",
      "fairly  :  fairli\n"
     ]
    }
   ],
   "source": [
    "words= [\"wait\", \"waiting\", \"waited\", \"waits\", \"listening\", \"history\", \"abitlity\", \"arrival\", \"finally\", \"congratulations\", \\\n",
    "       \"exaggeration\", \"understandable\", \"probability\", \"player\",\"toys\", \"consumer\", \"fairly\"]\n",
    "porter =PorterStemmer()\n",
    "for word in words:\n",
    "    rootWord=porter.stem(word)\n",
    "    print(word,\" : \",rootWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c43ebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "41707426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait  :  wait\n",
      "waiting  :  wait\n",
      "waited  :  wait\n",
      "waits  :  wait\n",
      "listening  :  list\n",
      "history  :  hist\n",
      "abitlity  :  abitl\n",
      "arrival  :  ar\n",
      "finally  :  fin\n",
      "congratulations  :  congrat\n",
      "exaggeration  :  exag\n",
      "understandable  :  understand\n",
      "probability  :  prob\n",
      "player  :  play\n",
      "toys  :  toy\n",
      "consumer  :  consum\n",
      "fairly  :  fair\n"
     ]
    }
   ],
   "source": [
    "lancaster =LancasterStemmer()\n",
    "for word in words:\n",
    "    rootWord=lancaster.stem(word)\n",
    "    print(word,\" : \",rootWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "259cb315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d658f983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait  :  wait\n",
      "waiting  :  wait\n",
      "waited  :  waited\n",
      "waits  :  wait\n",
      "listening  :  listen\n",
      "history  :  history\n",
      "abitlity  :  abitlity\n",
      "arrival  :  arrival\n",
      "finally  :  finally\n",
      "congratulations  :  congratulation\n",
      "exaggeration  :  exaggeration\n",
      "understandable  :  understand\n",
      "probability  :  probability\n",
      "player  :  play\n",
      "toys  :  toy\n",
      "consumer  :  consum\n",
      "fairly  :  fairly\n"
     ]
    }
   ],
   "source": [
    "regex =RegexpStemmer(\"ing$|es$|s$|er$|able$\")\n",
    "\n",
    "for word in words:\n",
    "    rootWord=regex.stem(word)\n",
    "    print(word,\" : \",rootWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "90ee5a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3653aa21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait  :  wait\n",
      "waiting  :  wait\n",
      "waited  :  wait\n",
      "waits  :  wait\n",
      "listening  :  listen\n",
      "history  :  histori\n",
      "abitlity  :  abitl\n",
      "arrival  :  arriv\n",
      "finally  :  final\n",
      "congratulations  :  congratul\n",
      "exaggeration  :  exagger\n",
      "understandable  :  understand\n",
      "probability  :  probabl\n",
      "player  :  player\n",
      "toys  :  toy\n",
      "consumer  :  consum\n",
      "fairly  :  fair\n"
     ]
    }
   ],
   "source": [
    "snow =SnowballStemmer(language=\"english\")\n",
    "\n",
    "for word in words:\n",
    "    rootWord=snow.stem(word)\n",
    "    print(word,\" : \",rootWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1179df",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Ref: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "\n",
    "https://www.nltk.org/_modules/nltk/stem/wordnet.html\n",
    "\n",
    "https://medium.com/@yashj302/lemmatization-f134b3089429\n",
    "\n",
    "https://www.geeksforgeeks.org/python-lemmatization-approaches-with-examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "84d2ecc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ami.munshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Ami.munshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Download Wordnet through NLTK in python console:\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9262a6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait: wait\n",
      "waiting: waiting\n",
      "waited: waited\n",
      "waits: wait\n",
      "listening: listening\n",
      "history: history\n",
      "abitlity: abitlity\n",
      "arrival: arrival\n",
      "finally: finally\n",
      "congratulations: congratulation\n",
      "exaggeration: exaggeration\n",
      "understandable: understandable\n",
      "probability: probability\n",
      "player: player\n",
      "toys: toy\n",
      "consumer: consumer\n",
      "fairly: fairly\n"
     ]
    }
   ],
   "source": [
    "#Using nltk WordNet Lemmatizer\n",
    "\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "words= [\"wait\", \"waiting\", \"waited\", \"waits\", \"listening\", \"history\", \"abitlity\", \"arrival\", \"finally\", \"congratulations\", \\\n",
    "       \"exaggeration\", \"understandable\", \"probability\", \"player\",\"toys\", \"consumer\", \"fairly\"]\n",
    "for word in words:\n",
    "    print(\"{}: {}\".format(word, lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63bee033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'students', 'are', 'waiting', 'outside', 'the', 'class', 'room']\n"
     ]
    }
   ],
   "source": [
    "sentence=\"The students are waiting outside the class room\"\n",
    "#Tokenizing the sentence\n",
    "word_tokens= word_tokenize(sentence)\n",
    "print(word_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef8a1e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'student', 'are', 'waiting', 'outside', 'the', 'class', 'room']\n"
     ]
    }
   ],
   "source": [
    "#Lemmatizing the tokens\n",
    "lemmatized=[]\n",
    "for token in word_tokens:\n",
    "    lemmatized.append(lemmatizer.lemmatize(token))\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1653d4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The student are waiting outside the class room\n"
     ]
    }
   ],
   "source": [
    "#Joining the lemmatized words\n",
    "lemmatized_sent= \" \".join(lemmatized)\n",
    "print(lemmatized_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee30314",
   "metadata": {},
   "source": [
    "Note: We can observe that the words are not appropriately lemmatized. Eg, are does not become \"be\" or waiting does not become \"wait\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfe991ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "#using POS tag\n",
    "print(lemmatizer.lemmatize(\"waiting\", 'v'))\n",
    "print(lemmatizer.lemmatize(\"are\", 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19ebfe49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('waiting', 'VBG')]\n"
     ]
    }
   ],
   "source": [
    "#Using nltk.pos_tag() method to tag words in large corpus\n",
    "\n",
    "print(nltk.pos_tag([\"waiting\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4486821",
   "metadata": {},
   "source": [
    "Refer to this website for POS tag\n",
    "\n",
    "https://cs.nyu.edu/~grishman/jet/guide/PennPOS.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fae0010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('students', 'NNS'), ('are', 'VBP'), ('waiting', 'VBG'), ('outside', 'IN'), ('the', 'DT'), ('class', 'NN'), ('room', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "sentence=\"The students are waiting outside the class room\"\n",
    "word_tokens= word_tokenize(sentence)\n",
    "print(nltk.pos_tag(word_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc418b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"are\", get_wordnet_pos(\"are\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c6c5122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'V'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag([\"waiting\"])[0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "47701ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(word):\n",
    "    nltk_tag = nltk.pos_tag([word])[0][1][0]\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1843465f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.ADJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4b334a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagger(\"are\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e9143082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"waiting\",pos_tagger(\"waiting\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "80dc3fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'student', 'be', 'wait', 'outside', 'the', 'class', 'room', 'eagerly']\n"
     ]
    }
   ],
   "source": [
    "sentence=\"The students are waiting outside the class room eagerly\"\n",
    "word_tokens= word_tokenize(sentence)\n",
    "lemmatized=[]\n",
    "for token in word_tokens:\n",
    "    tag= pos_tagger(token)\n",
    "    if tag==None:\n",
    "        lemmatized.append(lemmatizer.lemmatize(token))\n",
    "    else:\n",
    "        lemmatized.append(lemmatizer.lemmatize(token,tag))\n",
    "    \n",
    "    \n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1965a267",
   "metadata": {},
   "source": [
    "Here we note that adverb eagerly is not getting lemmatized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "545f5215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eagerly', 'RB')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag([\"eagerly\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "13281deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagger(\"eagerly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "32184f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nicely'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"nicely\",\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b05aeb",
   "metadata": {},
   "source": [
    "Ref: for adverb lemmatization\n",
    "    \n",
    "https://stackoverflow.com/questions/28475620/wordnet-lemmatizer-in-nltk-is-not-working-for-adverbs  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0a481164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'angry'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synset('angrily.r.1').lemmas()[0].pertainyms()[0].name()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6520cd53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eager'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('eagerly.r.1').lemmas()[0].pertainyms()[0].name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3fcbc8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'student', 'be', 'wait', 'outside', 'the', 'class', 'room', 'eager']\n"
     ]
    }
   ],
   "source": [
    "#This code will lemmatize adverbs also\n",
    "\n",
    "sentence=\"The students are waiting outside the class room eagerly\"\n",
    "word_tokens= word_tokenize(sentence)\n",
    "lemmatized=[]\n",
    "for token in word_tokens:\n",
    "    tag= pos_tagger(token)\n",
    "    if tag==None:\n",
    "        lemmatized.append(lemmatizer.lemmatize(token))\n",
    "    elif tag==\"r\":\n",
    "        lemmatized.append(wn.synset('{}.r.1'.format(token)).lemmas()[0].pertainyms()[0].name())\n",
    "        \n",
    "    else:\n",
    "        lemmatized.append(lemmatizer.lemmatize(token,tag))\n",
    "    \n",
    "    \n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb021ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
